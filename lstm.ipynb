{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "from platform import python_version\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "def data_generator(data):\n",
    "    h, w = data.shape\n",
    "    # print(h, w)\n",
    "\n",
    "    shape = (6,w)\n",
    "    new_data = sliding_window_view(data, shape)\n",
    "    n, *_ = new_data.shape\n",
    "    new_data = new_data.reshape((n,6,w))\n",
    "    return new_data\n",
    "\n",
    "def transform_data(arr):\n",
    "    x_arr = arr[:,:-1]\n",
    "    y_arr = arr[:,-1,-3]\n",
    "    return x_arr, y_arr\n",
    "\n",
    "#反标准化\n",
    "def inverse_label(data, width):\n",
    "    l, *_ = data.shape\n",
    "    data_trans = np.zeros((l, width))\n",
    "    data_trans[:,-3] = data.reshape((l))\n",
    "    data_trans = scaler.inverse_transform(data_trans)\n",
    "    return data_trans[:,-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('per_person_data/out_AS14.23.csv')\n",
    "df = df.dropna(thresh=2, subset=['activity', 'call', 'mood', 'screen'])\n",
    "df = df.dropna(how='all', subset=['appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather'])\n",
    "df = df.sort_values('time')\n",
    "print(df)\n",
    "df = df.fillna({'call':0,'sms':0, 'appCat.builtin':0, 'appCat.communication':0, 'appCat.unknown':0,\n",
    "       'appCat.entertainment':0, 'appCat.other':0, 'appCat.social':0, 'appCat.finance':0, 'appCat.game':0, 'appCat.office':0, 'appCat.travel':0, 'appCat.utilities':0, 'appCat.weather':0,  'appCat.weather':0})\n",
    "# df = df.drop(columns=['appCat.finance', 'appCat.game', 'appCat.office', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather'])\n",
    "df = df.drop(columns=['appCat.unknown'])\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "# print(df.shape)\n",
    "# df.drop_duplicates(subset=['time'])\n",
    "df = df.iloc[: , 1:]\n",
    "df_no_time = df.select_dtypes(include=['int64','float64'])\n",
    "print(df_no_time.shape)\n",
    "# print(df_no_time)\n",
    "# df_no_time.columns\n",
    "data = df_no_time.to_numpy()\n",
    "# data = df_no_time.to_numpy()\n",
    "# # print(data)\n",
    "# h, w = data.shape\n",
    "# # print(h, w)\n",
    "\n",
    "# shape = (6,w)\n",
    "# new_data = sliding_window_view(data, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scaling \n",
    "# Fill in nan with all dataset's median\n",
    "from sklearn.impute import SimpleImputer\n",
    "empty = []\n",
    "all_data = np.array(empty)\n",
    "\n",
    "files = sorted(glob.glob('per_person_data/*.csv'))\n",
    "# files = sorted(glob.glob('test/*.csv'))\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    df = df.dropna(thresh=2, subset=['activity', 'call', 'mood', 'screen'])\n",
    "    df = df.dropna(how='all', subset=['appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather'])\n",
    "    df = df.sort_values('time')\n",
    "    # df = df.fillna({'call':0,'sms':0})\n",
    "    # df = df.dropna(thresh=int(0.5*len(df)), axis=1)\n",
    "    df = df.fillna({'call':0,'sms':0, 'appCat.builtin':0, 'appCat.communication':0, 'appCat.entertainment':0, 'appCat.other':0, 'appCat.social':0})\n",
    "    df = df.drop(columns=['appCat.finance', 'appCat.game', 'appCat.office', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather'])\n",
    "    # df = df.fillna(df.mean(numeric_only=True))\n",
    "    df = df.iloc[: , 1:]\n",
    "    df_no_time = df.select_dtypes(include=['int64','float64'])\n",
    "    data = df_no_time.to_numpy()\n",
    "    \n",
    "    new_data = data_generator(data)\n",
    "    \n",
    "    if all_data.size == 0:\n",
    "        all_data = new_data\n",
    "    else:\n",
    "        all_data = np.concatenate((all_data, new_data), axis=0)    \n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "nsamples, nx, ny = all_data.shape\n",
    "all_data = all_data.reshape((nsamples*nx, ny))\n",
    "im = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "all_data = im.fit_transform(all_data)\n",
    "trans_data = scaler.fit_transform(all_data).reshape((nsamples, nx, ny))\n",
    "all_data = all_data.reshape((nsamples, nx, ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Preprocessing & Feature Engineering Revamped\n",
    "'''\n",
    "\n",
    "df_all = pd.DataFrame() \n",
    "\n",
    "files = sorted(glob.glob('per_person_data/*.csv'))\n",
    "for f in files:\n",
    "    # Initial filtering & preprocessing\n",
    "    df = pd.read_csv(f)\n",
    "    df = df[df['mood'].notna()]\n",
    "    df = df.sort_values('time')\n",
    "    df.drop_duplicates(subset=['time'])\n",
    "    df = df.iloc[: , 1:] \n",
    "\n",
    "    # NOTE: Turn this on for extra features \n",
    "    if False:\n",
    "        mood = np.array(df['circumplex.arousal'])\n",
    "        df['past_mood'] = [np.nan] + [np.median(mood[i]) for i in range(len(mood) - 1)]\n",
    "        df['past_mood_std'] = [np.nan] + [np.max(mood[i: i+5]) - np.min(mood[i: i+5]) for i in range(len(mood) - 6)]\n",
    "\n",
    "    # NOTE: SLOW for large dataframes\n",
    "    df_all = pd.concat([df_all, df], axis=0)  \n",
    "    \n",
    "\n",
    "df_all = df_all.drop(['time'], axis=1)\n",
    "df_all['call'] = df_all['call'].fillna(0)\n",
    "df_all['sms'] = df_all['sms'].fillna(0)\n",
    "\n",
    "for col in df_all.columns:\n",
    "    prc_nan = df_all[col].isnull().sum() / len(df_all[col])\n",
    "    \n",
    "    if prc_nan < .4:\n",
    "        df_all[col] = df_all[col].fillna(df_all[col].median())\n",
    "    else:\n",
    "        df_all = df_all.drop([col], axis=1)\n",
    "\n",
    "data = df_all.to_numpy()\n",
    "all_data = data_generator(data)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "nsamples, nx, ny = all_data.shape\n",
    "trans_data = all_data.reshape((nsamples*nx, ny))\n",
    "trans_data = scaler.fit_transform(trans_data).reshape((nsamples, nx, ny))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, regularizers, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from attention import Attention\n",
    "\n",
    "# divide dataset\n",
    "X, Y = transform_data(trans_data)\n",
    "\n",
    "#Generate indices on the row-wise length of the whole \n",
    "#array\n",
    "fold_indices = np.arange(X.shape[0])\n",
    "\n",
    "#Shuffle the indices -- if you want, but not \n",
    "#neccessary\n",
    "np.random.shuffle(fold_indices)\n",
    "\n",
    "k = 8\n",
    "#Split the indices into k-parts (returns a list of \n",
    "#numpy arrays)\n",
    "cross_validatoin_score = []\n",
    "test_all_x = np.array(empty)\n",
    "test_all_y = np.array(empty)\n",
    "eval_indices = np.array_split(fold_indices, k)\n",
    "for idx, e in enumerate(eval_indices):\n",
    "    #Define the evaluation set for the current fold\n",
    "    test_set_x = X[e]\n",
    "    test_set_y = Y[e]\n",
    "\n",
    "    #exclude the upon parts indices from the \n",
    "    #whole array (similarly on the upon answers)\n",
    "    \n",
    "    mask_eval = np.ones(X.shape[0], bool)\n",
    "    \n",
    "    #Set indices of the eval set to false\n",
    "    mask_eval[e] = False\n",
    "\n",
    "    #Subset by the bool array:\n",
    "    train_set_x = X[mask_eval].astype('float32')\n",
    "    train_set_y = Y[mask_eval].astype('float32')\n",
    "    print(f'Fold: {idx}')\n",
    "    print(f'Train set shape: {train_set_x.shape}    Test set shape: {test_set_x.shape}')\n",
    "    num_samples, time_steps, input_dim = train_set_x.shape\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(time_steps, input_dim)))\n",
    "    model.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "    model.add(Bidirectional(LSTM(128,return_sequences=False)))\n",
    "    model.add(Dropout(rate=0.7))\n",
    "    # model.add(Attention(units=32))\n",
    "    # model.add(Dense(4,activation=\"tanh\",  kernel_regularizer=regularizers.l1(0.01)))\n",
    "    # model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(1,activation=\"tanh\",  kernel_regularizer=regularizers.l1(0.01)))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=0.01, clipvalue=0.5)\n",
    "    model.compile(optimizer=optimizer,loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
    "    model.fit(train_set_x,train_set_y,batch_size=32,epochs=7,validation_split=0.1,verbose=1)\n",
    "    # Evaluate the model on the test data using `evaluate`\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate(test_set_x, test_set_y, batch_size=64)\n",
    "    # print(\"test loss, test acc:\", results)\n",
    "    cross_validatoin_score.append(results)\n",
    "\n",
    "    if test_all_x.size == 0:\n",
    "        test_all_x = test_set_x\n",
    "        test_all_y = test_set_y\n",
    "    else:\n",
    "        test_all_x = np.concatenate((test_all_x, test_set_x), axis=0)\n",
    "        test_all_y = np.concatenate((test_all_y, test_set_y), axis=0)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "_, _, w = train_set_x.shape\n",
    "#预测测试集\n",
    "# predict_trainY=model.predict(train_set_x)\n",
    "predict_testY=model.predict(test_set_x)\n",
    "#反标准化\n",
    "# trainY=scaler.inverse_transform(predict_trainY)\n",
    "testY_real = inverse_label(test_set_y[:,np.newaxis], w)\n",
    "testY_predict = inverse_label(predict_testY, w)\n",
    "# testY_real = Y_test\n",
    "# testY_predict = predict_testY\n",
    "print(model.summary())\n",
    "print(np.mean(np.array(cross_validatoin_score),axis=0))\n",
    "\n",
    "#看一看数据形状\n",
    "print(\"Y:\",testY_predict.reshape((-1)),testY_predict.shape)\n",
    "print(\"Y_real:\",testY_real,testY_real.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(testY_predict,\"r\",label=\"Predict\")\n",
    "plt.plot(testY_real,\"b\",label=\"Real\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "predict_trainY=model.predict(train_set_x)\n",
    "#反标准化\n",
    "trainY_real = inverse_label(train_set_y[:,np.newaxis], w)\n",
    "trainY_predict = inverse_label(predict_trainY,w)\n",
    "\n",
    "#看一看数据形状\n",
    "# print(\"Y:\",trainY_predict.reshape((-1)),trainY_predict.shape)\n",
    "# print(\"Y_real:\",trainY_real,trainY_real.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(trainY_real,\"b\",label=\"Real\")\n",
    "plt.plot(trainY_predict,\"r\",label=\"Predict\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML LR\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# divide dataset\n",
    "X, Y = transform_data(all_data)\n",
    "nsamples, nx, ny = X.shape\n",
    "X = X.reshape((nsamples,nx*ny))\n",
    "k = 8\n",
    "lr=LinearRegression()\n",
    "kf=KFold(n_splits=k, shuffle=True, random_state=0)\n",
    "score=cross_val_score(lr,X,Y,cv=kf,scoring='neg_mean_squared_error')\n",
    "print(\"cross Validation scores: {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(np.negative(score.mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Rigid\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "# divide dataset\n",
    "X, Y = transform_data(all_data)\n",
    "nsamples, nx, ny = X.shape\n",
    "X = X.reshape((nsamples,nx*ny))\n",
    "\n",
    "ridge=RidgeCV(alphas=np.arange(1,1001,100))\n",
    "kf=KFold(n_splits=k, shuffle=True, random_state=0)\n",
    "score=cross_val_score(ridge,X,Y,cv=kf,scoring='r2')\n",
    "print(\"cross Validation scores: {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(np.negative(score.mean())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Lasso\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LassoCV\n",
    "# divide dataset\n",
    "X, Y = transform_data(all_data)\n",
    "nsamples, nx, ny = X.shape\n",
    "X = X.reshape((nsamples,nx*ny))\n",
    "\n",
    "lasso=LassoCV()\n",
    "kf=KFold(n_splits=k, shuffle=True, random_state=0)\n",
    "score=cross_val_score(lasso,X,Y,cv=kf,scoring='r2')\n",
    "print(\"cross Validation scores: {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(np.negative(score.mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML SVR\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# divide dataset\n",
    "X, Y = transform_data(trans_data)\n",
    "\n",
    "k = 8\n",
    "# divide dataset\n",
    "X, Y = transform_data(all_data)\n",
    "nsamples, nx, ny = X.shape\n",
    "X = X.reshape((nsamples,nx*ny))\n",
    "clf = SVR(kernel='linear')\n",
    "kf=KFold(n_splits=k, shuffle=True, random_state=0)\n",
    "score=cross_val_score(clf,X,Y,cv=kf,scoring='neg_mean_squared_error')\n",
    "print(\"cross Validation scores: {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(np.negative(score.mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML KNN\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# divide dataset\n",
    "X, Y = transform_data(trans_data)\n",
    "nsamples, nx, ny = X.shape\n",
    "X = X.reshape((nsamples,nx*ny))\n",
    "neigh = KNeighborsRegressor(n_neighbors=2)\n",
    "kf=KFold(n_splits=k, shuffle=True, random_state=0)\n",
    "score=cross_val_score(neigh,X,Y,cv=kf,scoring='neg_mean_squared_error')\n",
    "print(\"cross Validation scores: {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(np.negative(score.mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML RandomForestReg\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "X, Y = transform_data(all_data)\n",
    "nsamples, nx, ny = X.shape\n",
    "X = X.reshape((nsamples,nx*ny))\n",
    "regr = RandomForestRegressor(max_depth=15, random_state=0, n_estimators=50, criterion='squared_error')\n",
    "kf=KFold(n_splits=k, shuffle=True, random_state=0)\n",
    "score=cross_val_score(regr,X,Y,cv=kf,scoring='r2')\n",
    "print(\"cross Validation scores: {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(np.negative(score.mean())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(max_depth = 12,\n",
    "                             learning_rate = 1e-1,\n",
    "                             n_estimators = 50,\n",
    "                             objective = 'reg:squarederror',\n",
    "                             n_jobs = -1)\n",
    "\n",
    "kf=KFold(n_splits=k, shuffle=True, random_state=0)\n",
    "score=cross_val_score(xgb_model,X,Y,cv=kf,scoring='r2')\n",
    "print(\"cross Validation scores: {}\".format(score))\n",
    "print(\"Average Cross Validation score :{}\".format(np.negative(score.mean())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "Y_train = Y_train.astype('float32')\n",
    "num_samples, time_steps, input_dim = X_train.shape\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(time_steps, input_dim)))\n",
    "model.add(Bidirectional(LSTM(32,return_sequences=True)))\n",
    "# model.add(LSTM(16,return_sequences=False))\n",
    "model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "# model.add(Attention(name='attention_weight'))\n",
    "model.add(Dense(4,activation=\"tanh\",  kernel_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(1,activation=\"tanh\",  kernel_regularizer=regularizers.l1(0.01)))\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.01, clipvalue=0.5)\n",
    "model.compile(optimizer=optimizer,loss=\"mean_squared_error\", metrics=[\"mse\"])\n",
    "model.fit(X_train,Y_train,batch_size=64,epochs=7,validation_split=0.1,verbose=1)\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "# print(f'Evaluate: {model.evaluate(X_test,Y_test, verbose=1)}')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#反标准化\n",
    "def inverse_label(data, width):\n",
    "    l, _ = data.shape\n",
    "    data_trans = np.zeros((l, width))\n",
    "    data_trans[:,-3] = data.reshape((l))\n",
    "    data_trans = scaler.inverse_transform(data_trans)\n",
    "    return data_trans[:,-3]\n",
    "\n",
    "h, _, w = X_train.shape\n",
    "#预测测试集\n",
    "predict_trainY=model.predict(X_train)\n",
    "predict_testY=model.predict(X_test)\n",
    "#反标准化\n",
    "# trainY=scaler.inverse_transform(predict_trainY)\n",
    "testY_real = inverse_label(Y_test[:,np.newaxis], w)\n",
    "testY_predict = inverse_label(predict_testY, w)\n",
    "# testY_real = Y_test\n",
    "# testY_predict = predict_testY\n",
    "\n",
    "\n",
    "#看一看数据形状\n",
    "print(\"Y:\",testY_predict.reshape((-1)),testY_predict.shape)\n",
    "print(\"Y_real:\",testY_real,testY_real.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(testY_predict,\"b\",label=\"Predict\")\n",
    "plt.plot(testY_real,\"r\",label=\"Real\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#看一看数据形状\n",
    "# trainY_predict = inverse_label(predict_trainY, w)\n",
    "# trainY_real = inverse_label(Y_train[:,np.newaxis], w)\n",
    "trainY_predict = predict_trainY\n",
    "trainY_real = Y_train\n",
    "\n",
    "print(\"Y:\",trainY_predict.reshape((-1)),trainY_predict.shape)\n",
    "print(\"Y_real:\",trainY_real,trainY_real.shape)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(trainY_predict,\"b\",label=\"Predict\")\n",
    "plt.plot(trainY_real,\"r\",label=\"Real\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers,Input,optimizers\n",
    "# from tensorflow.keras.layers import Bidirectional\n",
    "# input_shape=Input(shape=(x_train.shape[1],x_train.shape[2]))\n",
    "# lstm1=layers.LSTM(32,return_sequences=1)(input_shape)\n",
    "# print(\"lstm1:\",lstm1.shape)\n",
    "# lstm2=layers.LSTM(64,return_sequences=0)(lstm1)\n",
    "# print(\"lstm2:\",lstm2.shape)\n",
    "# dense1=layers.Dense(64,activation=\"relu\")(lstm2)\n",
    "# print(\"dense:\",dense1.shape)\n",
    "# dropout=layers.Dropout(rate=0.2)(dense1)\n",
    "# print(\"dropout:\",dropout.shape)\n",
    "# ouput_shape=layers.Dense(1,activation=\"relu\")(dropout)\n",
    "# model=tf.keras.Model(input_shape,ouput_shape)\n",
    "# model.compile(loss=\"mean_squared_error\",optimizer=\"Adam\",metrics=[\"mse\"])#mse作为l损失函数，采用Adam作为寻优方式\n",
    "# history=model.fit(x_train,y_train,batch_size=32,epochs=10,validation_split=0.1,verbose=1)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "print(is_cuda)\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ee4d181cff9adcd5efc9b7e40de0ada85f5a4786332094f7b11b4a1f550d203"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
